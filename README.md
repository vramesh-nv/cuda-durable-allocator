# CUDA FUSE: GPU Memory Sharing via Filesystem

This README was generated by cursor for the following prompt: "Take a look at the current implementation and tests and write the README for this git repo"

A FUSE-based filesystem that enables GPU memory sharing between processes using CUDA Fabric Handles. This prototype demonstrates inter-process GPU memory sharing through a filesystem interface, making GPU allocations accessible via standard file operations and extended attributes.

## Overview

This project creates a virtual filesystem where files represent GPU memory allocations. When a file is created and truncated to a specific size, the FUSE driver allocates corresponding GPU memory using CUDA Virtual Memory Management (VMM) APIs. The GPU memory can then be shared between processes via CUDA Fabric Handles stored as extended attributes.

### Key Concepts

- **Files as GPU Allocations**: Each file in the mounted filesystem represents a GPU memory allocation
- **Truncate-to-Allocate**: Use `truncate()` to trigger GPU memory allocation of specific size
- **Fabric Handle Sharing**: CUDA Fabric Handles enable cross-process GPU memory sharing
- **Extended Attributes**: Access allocation metadata and fabric handles via `getxattr()`/`setxattr()`

## Architecture

```
┌─────────────────┐    ┌─────────────────┐
│   Process A     │    │   Process B     │
│                 │    │                 │
│ 1. truncate()   │    │ 4. getxattr()   │
│ 2. getxattr()   │    │ 5. cuMemImport..│
│ 3. cuMemMap()   │    │ 6. cuMemMap()   │
└─────────┬───────┘    └─────────┬───────┘
          │                      │
          │  ┌─────────────────┐ │
          └──┤  FUSE Driver    ├─┘
             │                 │
             │ • File creation │
             │ • GPU alloc     │
             │ • Fabric handle │
             │ • Extended attr │
             └─────────────────┘
                      │
             ┌─────────────────┐
             │  CUDA Driver    │
             │                 │
             │ • cuMemCreate   │
             │ • cuMemExport   │
             │ • cuMemImport   │
             └─────────────────┘
```

## Dependencies

### System Requirements
- Linux with FUSE support
- NVIDIA GPU with CUDA capability >= 6.0
- CUDA Toolkit 11.0 or newer

### Development Dependencies
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install libfuse3-dev libglib2.0-dev build-essential pkg-config

# CentOS/RHEL
sudo dnf install fuse3-devel glib2-devel gcc gcc-c++ make pkgconfig

# Or use the provided script
./install_deps.sh
```

## Building

```bash
# Check all dependencies
make check-deps

# Build both FUSE driver and test client
make

# Build with debug symbols
make debug

# Clean build
make clean
```

## Usage

### 1. Starting the FUSE Filesystem

```bash
# Create mount point
mkdir ./test_mount

# Start filesystem in foreground (for debugging)
./build/gpu_mem_fuse ./test_mount -f -d

# Or start in background
./build/gpu_mem_fuse ./test_mount
```

### 2. Basic GPU Memory Operations

```bash
# Create a file (no GPU memory yet)
touch ./test_mount/my_buffer

# Allocate 8MB of GPU memory by truncating
truncate -s 8M ./test_mount/my_buffer

# Check allocation info via extended attributes
getfattr -d ./test_mount/my_buffer

# Get allocation size
getfattr -n user.allocation_size ./test_mount/my_buffer

# List all GPU allocations
ls -la ./test_mount/
```

### 3. Cross-Process Memory Sharing

The test client demonstrates parent-child process sharing:

```bash
# Terminal 1: Parent process (creates and writes to GPU memory)
./build/test_client --parent

# Terminal 2: Child process (reads from shared GPU memory)  
./build/test_client --child
```

### 4. Manual Cross-Process Sharing

```bash
# Process A: Create and allocate
touch ./test_mount/shared_buffer
truncate -s 16M ./test_mount/shared_buffer

# Process B: Access the fabric handle
getfattr -n user.fabric_handle ./test_mount/shared_buffer
# Returns binary fabric handle data

# Process B can now import this handle using cuMemImportFromShareableHandle()
```

## API Usage in Applications

### C/C++ Example

```c
#include <sys/xattr.h>
#include <sys/stat.h>
#include <cuda.h>

// Create GPU allocation
const char *path = "./test_mount/gpu_buffer";
int fd = creat(path, 0644);
close(fd);

// Allocate 1MB GPU memory
truncate(path, 1024 * 1024);

// Get allocation size
char size_str[64];
ssize_t len = getxattr(path, "user.allocation_size", size_str, sizeof(size_str));

// Get fabric handle for sharing
CUmemFabricHandle fabric_handle;
ssize_t handle_size = getxattr(path, "user.fabric_handle", 
                              &fabric_handle, sizeof(fabric_handle));

// Import the handle in another process
CUmemGenericAllocationHandle gpu_handle;
cuMemImportFromShareableHandle(&gpu_handle, &fabric_handle, CU_MEM_HANDLE_TYPE_FABRIC);

// Map to virtual address
CUdeviceptr va;
cuMemAddressReserve(&va, allocation_size, granularity, 0, 0);
cuMemMap(va, allocation_size, 0, gpu_handle, 0);

// Set access permissions
CUmemAccessDesc accessDesc = {
    .location = {.type = CU_MEM_LOCATION_TYPE_DEVICE, .id = 0},
    .flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE
};
cuMemSetAccess(va, allocation_size, &accessDesc, 1);
```

### Python Example

```python
import os
import xattr

# Create file and allocate GPU memory
path = "./test_mount/python_buffer"
with open(path, 'w') as f:
    pass

# Allocate 2MB
os.truncate(path, 2 * 1024 * 1024)

# Get allocation metadata
size_bytes = xattr.getxattr(path, "user.allocation_size")
print(f"Allocated: {size_bytes.decode()} bytes")

# Get fabric handle (binary data)
fabric_handle = xattr.getxattr(path, "user.fabric_handle")
print(f"Fabric handle size: {len(fabric_handle)} bytes")
```

## Test Client Details

The included test client (`test_client.cu`) demonstrates a complete workflow:

### Parent Process
1. Creates a file in the FUSE filesystem
2. Truncates to 8MB to trigger GPU allocation
3. Retrieves fabric handle via `getxattr()`
4. Maps GPU memory using CUDA VMM APIs
5. Writes test pattern to GPU memory using a CUDA kernel

### Child Process  
1. Finds the existing allocation
2. Retrieves the same fabric handle
3. Imports and maps the GPU memory
4. Reads and validates the test pattern using a CUDA kernel

### Running the Test

```bash
# Method 1: Automated test
make test-client

# Method 2: Manual two-terminal test
# Terminal 1:
./build/test_client --parent

# Terminal 2: 
./build/test_client --child
```

## Implementation Details

### CUDA Fabric Handles

The system uses CUDA Fabric Handles for memory sharing:

```c
// Export (in FUSE driver)
CUmemFabricHandle fabricHandle;
cuMemExportToShareableHandle(&fabricHandle, gpu_handle, CU_MEM_HANDLE_TYPE_FABRIC, 0);

// Import (in application)
CUmemGenericAllocationHandle imported_handle;
cuMemImportFromShareableHandle(&imported_handle, &fabricHandle, CU_MEM_HANDLE_TYPE_FABRIC);
```

### Extended Attributes

The FUSE driver exposes two extended attributes:

- **`user.allocation_size`**: Size of GPU allocation in bytes (string)
- **`user.fabric_handle`**: Binary fabric handle data (64 bytes)

### File Lifecycle

1. **File Creation**: `touch` or `creat()` creates file entry, no GPU memory
2. **Memory Allocation**: `truncate()` with size > 0 triggers GPU allocation
3. **Handle Generation**: Fabric handle created automatically during allocation
4. **Memory Deallocation**: `truncate()` with size = 0 deallocates GPU memory
5. **File Removal**: `unlink()` removes file and deallocates any GPU memory

### Thread Safety

- Each file has its own mutex for thread-safe operations
- Global context protected by a separate mutex
- CUDA operations are inherently thread-safe within contexts

## Current Limitations

1. **Single GPU**: Only supports device 0
2. **No Persistence**: Allocations don't survive filesystem restarts
3. **No Memory Management**: No automatic cleanup or garbage collection
4. **Limited Error Handling**: Basic error reporting only
5. **No Access Control**: All processes can access all allocations

## Debugging

### Verbose Mode
```bash
./build/gpu_mem_fuse ./test_mount -f -d
```

### Common Issues

1. **"CUDA error" during allocation**: Check GPU memory availability
2. **"Permission denied"**: Ensure user has access to CUDA devices
3. **"No such file"**: Mount point may not exist or FUSE not running
4. **Compilation errors**: Verify CUDA toolkit installation and paths

### Cleanup
```bash
# Unmount filesystem
fusermount3 -u ./test_mount

# Or use make target
make test-clean
```

## Future Enhancements

- [ ] Multi-GPU support
- [ ] Persistent allocation registry  
- [ ] Memory usage monitoring and limits
- [ ] Access control and permissions
- [ ] Direct `mmap()` support for GPU memory
- [ ] Integration with CUDA memory pools
- [ ] Compression for large allocations
- [ ] Network sharing capabilities

## Development

### Code Structure

```
├── gpu_mem_fuse.c     # Main FUSE driver implementation
├── gpu_mem_fuse.h     # Header with data structures  
├── test_client.cu     # CUDA test client for validation
├── Makefile           # Build system
└── install_deps.sh    # Dependency installation script
```

### Key Functions

- `gpu_fuse_truncate()`: Handles GPU memory allocation/deallocation
- `gpu_fuse_getxattr()`: Exposes fabric handles and metadata
- `gpu_fuse_create()`: Creates file entries
- `gpu_fuse_unlink()`: Cleanup and deallocation

### Testing New Features

```bash
# Build and test changes
make clean && make debug
make test-client

# Or manual testing
./build/gpu_mem_fuse ./test_mount -f -d
# (in another terminal)
./build/test_client --parent
```

## Contributing

This is a research prototype. Key areas for contribution:

1. **Multi-GPU Support**: Extend to support multiple devices
2. **Error Handling**: Improve robustness and error messages
3. **Performance**: Optimize allocation/deallocation paths
4. **Testing**: Additional test cases and validation
5. **Documentation**: Usage examples and API documentation

## License

This project is provided as a research prototype for educational purposes. See individual files for specific license information.